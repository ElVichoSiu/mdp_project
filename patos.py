# -*- coding: utf-8 -*-
"""PATOS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RmRMrtZW2nTNopPq5rdBrBB88G9-tio4

Código para crear dataset apartir de datos de Wikipedia Laura Maldonado Lagos, Vicente Thiele Muñoz y Javiera Romero Orrego Grupo 2

Primero desde cada página de wikipedia extraemos la informacion
"""

import pandas as pd
url = 'https://en.wikipedia.org/wiki/List_of_fiction_works_made_into_feature_films_(0%E2%80%939,_A%E2%80%93C)'
pagina = pd.read_html(url)
lineas_relevantes = pagina[2:] #ignoramos las primeras lineas que son irrelevantes
df = pd.concat(lineas_relevantes, ignore_index=True)
df.columns = ['fiction_work', 'film_adaptation'] #le ponemos nombres a las columnas
df.to_csv('fiction_to_filmsA-C.csv', index=False) #guardamos la info al csv

"""Repetimos lo mismo para cada página"""

url = 'https://en.wikipedia.org/wiki/List_of_fiction_works_made_into_feature_films_(D%E2%80%93J)'
pagina = pd.read_html(url)
lineas_relevantes = pagina[2:]
df = pd.concat(lineas_relevantes, ignore_index=True)
df.columns = ['fiction_work', 'film_adaptation']
df.to_csv('fiction_to_filmsD-J.csv', index=False)

url = 'https://en.wikipedia.org/wiki/List_of_fiction_works_made_into_feature_films_(K%E2%80%93R)'
pagina = pd.read_html(url)
lineas_relevantes = pagina[3:]
df = pd.concat(lineas_relevantes, ignore_index=True)
df.columns = ['fiction_work', 'film_adaptation']
df.to_csv('fiction_to_filmsK-R.csv', index=False)

url = 'https://en.wikipedia.org/wiki/List_of_fiction_works_made_into_feature_films_(S%E2%80%93Z)'
pagina = pd.read_html(url)
lineas_relevantes = pagina[2:]
df = pd.concat(lineas_relevantes, ignore_index=True)
df.columns = ['fiction_work', 'film_adaptation']
df.to_csv('fiction_to_filmsS-Z.csv', index=False)

"""Ahora combinamos todos los csv en uno solo"""

import pandas as pd
import re
csv_files = [
    'fiction_to_filmsA-C.csv',
    'fiction_to_filmsD-J.csv',
    'fiction_to_filmsK-R.csv',
    'fiction_to_filmsS-Z.csv'
]

combined_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)
#habian algunos valores con formatos extraños al final tipo [N 10] por eso este map
combined_df = combined_df.applymap(
    lambda x: re.sub(r'\[N \d{1,2}\]', '', x).replace('"', '').strip() if isinstance(x, str) else x
)
#para extraer solo el titulo sin el (año)
def clean_title(title):
    return re.sub(r'\s*\(\d{4}\)\s*$', '', title).strip()
#
def extract_info(row):
    text = row['fiction_work']
    film = row.get('film_adaptation', '')
    series_match = re.match(r'^(.*?)\s*\((\d{4}–\d{4})\)\s*\(series\),\s*(.+)$', text)
    if series_match:
        _, date_range, author = series_match.groups()
        title = clean_title(film)
        return pd.Series([title, date_range, author])
    match = re.match(r"^(.*?)(?:\s*\(.*?\))?\s*\((\d{4})\),\s*(.*)$", text)
    if match:
        title, year, author = match.groups()
        return pd.Series([title.strip(), year, author.strip()])
    return pd.Series([text, None, None])


combined_df[['fiction_work', 'releaseDate', 'author']] = combined_df.apply(extract_info, axis=1)


combined_df.sort_values(by='fiction_work', inplace=True)
combined_df.to_csv('combined_fiction_to_films.csv', index=False)

"""Para conocer el tamañano del dataset creado"""

print(f"Cantidad de filas: {len(combined_df)}")

"""Para conocer la cantidad de libros únicos que tiene, ya que un libro puede estar asociado a 1 o más películas"""

import pandas as pd
df = pd.read_csv('combined_fiction_to_films.csv')
unique_fiction_df = df.drop_duplicates(subset='fiction_work')
unique_fiction_df.to_csv('unique_fiction_works.csv', index=False)
print(f"Cantidad de libros únicos: {len(unique_fiction_df)}")